{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pickle, sys, os, time, pandas as pd\n",
    "if 'E:\\\\Rashi\\\\octis\\\\OCTIS' not in sys.path: sys.path.append('E:\\\\Rashi\\\\octis\\\\OCTIS')\n",
    "\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "import spacy, string\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.lang.en import stop_words\n",
    "stop_words=stop_words.STOP_WORDS\n",
    "\n",
    "def addCustomDataset(datafile, column_name='content', basepath='E:\\\\Rashi\\\\octis\\\\OCTIS\\\\preprocessed_datasets', dataset_name=f'ds_{int(time.time())}', stop_words=stop_words, custom_stop_words=[]):\n",
    "\n",
    "    \"\"\"\n",
    "    Add custom dataset to octis dashboard\n",
    "\n",
    "    datafile: path of the data file.\n",
    "    type: str\n",
    "\n",
    "    basepath: folder path for preprocessed datasets in octis\n",
    "    type: str\n",
    "    default: E:\\\\Rashi\\\\octis\\\\OCTIS\\\\preprocessed_datasets\n",
    "\n",
    "    dataset_name: name of the dataset\n",
    "    type: str\n",
    "    default: ds_{int(time.time())}\n",
    "\n",
    "    custom_stop_words: dataset specific stop words\n",
    "    type: list\n",
    "    default: []\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if type(datafile)==str and datafile[-4:]=='.csv':\n",
    "        data=pd.read_csv(datafile)\n",
    "        data.drop_duplicates(inplace=True,ignore_index=True)\n",
    "        docs=[doc.replace('\\n',' ')+' \\n' for doc in data[column_name].to_list() if type(doc)==str]\n",
    "\n",
    "        path=datafile.split('\\\\')[:-1]\n",
    "        filename= 'E:\\\\Rashi\\\\octis\\\\helperFiles\\\\' + dataset_name + '_content.txt'\n",
    "        \n",
    "        try:\n",
    "            os.remove(filename)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        with open(filename,'a',encoding=\"utf-8\") as file:\n",
    "            file.writelines(docs)\n",
    "\n",
    "        stop_words=list(stop_words)+custom_stop_words\n",
    "        preprocessor = Preprocessing(punctuation=string.punctuation, stopword_list=stop_words, min_words_docs=50)\n",
    "        dataset = preprocessor.preprocess_dataset(documents_path=filename)\n",
    "    \n",
    "    elif type(datafile)==str and datafile[-4:]=='.txt':\n",
    "        stop_words=list(stop_words)+custom_stop_words\n",
    "        preprocessor = Preprocessing(punctuation=string.punctuation, stopword_list=stop_words, min_words_docs=50)\n",
    "        dataset = preprocessor.preprocess_dataset(documents_path=datafile) \n",
    "\n",
    "    elif type(datafile)==str and datafile[-4:]=='.pkl':\n",
    "        with open(datafile,'rb') as file:\n",
    "            dataset=pickle.load(file)\n",
    "\n",
    "    else:\n",
    "        raise TypeError('Inappropriate argument type for datafile. datafile should be a string and a file path')\n",
    "\n",
    "    #createDirectory\n",
    "    if os.path.exists(os.path.join(basepath,dataset_name)):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(os.path.join(basepath,dataset_name))\n",
    "\n",
    "    #metadataFile\n",
    "    metadata=dataset.get_metadata()\n",
    "    with open(os.path.join(basepath, dataset_name,'metadata.json'), 'w', encoding='utf-8') as file:\n",
    "        json.dump(dataset.get_metadata(), file)\n",
    "    print(f'saved metadata')\n",
    "\n",
    "    #corpus\n",
    "    corpus=dataset.get_corpus()\n",
    "    pd.DataFrame(corpus).to_csv(os.path.join(basepath, dataset_name,'corpus.tsv'),sep='\\t',index=False,header=False)\n",
    "    print(f'saved corpus')\n",
    "\n",
    "    #labels\n",
    "    labels=dataset.get_labels()\n",
    "    with open(os.path.join(basepath, dataset_name,'labels.txt'), 'w', encoding='utf-8') as file:\n",
    "        if len(labels)==0: print('labels not provided')\n",
    "        else: [file.write(f'{label} \\n') for label in labels]\n",
    "    print(f'saved labels')\n",
    "\n",
    "    #vocab\n",
    "    vocab=dataset.get_vocabulary()\n",
    "    with open(os.path.join(basepath, dataset_name,'vocabulary.txt'), 'w', encoding='utf-8') as file:\n",
    "        [file.write(f'{word} \\n') for word in vocab]\n",
    "    print(f'saved vocab')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [02:02<00:00,  4.56it/s]\n",
      "e:\\Rashi\\octis\\octis_pipeline\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:394: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['d', 'll', 'm', 'n', 's', 't', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab created. vocab size:  52926\n",
      "saved metadata\n",
      "saved corpus\n",
      "labels not provided\n",
      "saved labels\n",
      "saved vocab\n"
     ]
    }
   ],
   "source": [
    "datafile=\"E:\\\\Rashi\\\\datasets\\\\articles_apple.csv\"\n",
    "column_name='Content'\n",
    "basepath='E:\\\\Rashi\\\\datasets\\\\misc'\n",
    "dataset_name='apple'\n",
    "custom_stop_words=[]\n",
    "addCustomDataset(datafile=datafile, column_name=column_name, basepath=basepath, dataset_name=dataset_name, custom_stop_words=custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octis_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
