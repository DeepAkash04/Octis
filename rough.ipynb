{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = dict()\n",
    "# hyperparameters[\"n_topics\"] = 20\n",
    "# hyperparameters[\"n_iter\"] = 30\n",
    "\n",
    "# hyperparams = {}\n",
    "# train_corpus = dataset.get_corpus()\n",
    "# id2word = corpora.Dictionary(dataset.get_corpus())\n",
    "# id_corpus = [id2word.doc2bow(document) for document in train_corpus]\n",
    "# hyperparams[\"num_topics\"] = hyperparameters[\"num_topics\"]\n",
    "\n",
    "# # Allow alpha to be a float in case of symmetric alpha\n",
    "# if \"alpha\" in hyperparams:\n",
    "#     if isinstance(hyperparams[\"alpha\"], float): hyperparams[\"alpha\"] = [hyperparams[\"alpha\"]] * hyperparams[\"num_topics\"]\n",
    "\n",
    "# hyperparams[\"corpus\"] = id_corpus\n",
    "# hyperparams[\"id2word\"] = id2word\n",
    "# hyperparameters.update(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'E:\\\\Rashi\\\\octis\\\\OCTIS' not in sys.path: sys.path.append('E:\\\\Rashi\\\\octis\\\\OCTIS') \n",
    "\n",
    "from octis.dataset.dataset import Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.corpora as corpora\n",
    "dataset = Dataset()\n",
    "dataset.fetch_dataset('20NewsGroup')\n",
    "\n",
    "id2word = corpora.Dictionary(dataset.get_corpus())\n",
    "id_corpus = [id2word.doc2bow(document) for document in dataset.get_corpus()]\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "vectorizer = CountVectorizer(vocabulary=id2word.token2id)\n",
    "X = vectorizer.fit_transform([' '.join(text) for text in dataset.get_corpus()]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=100)\n",
    "dtm = model.fit_transform(X)\n",
    "ttm = model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16309, 20), (20, 1612))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape, ttm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16309, 1612)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(dtm,ttm).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadData\n",
    "import boto3, pandas as pd\n",
    "\n",
    "s3=boto3.client('s3')\n",
    "obj=s3.get_object(Bucket='topic-evolution-tracker',Key='preprocessed_datasets/articles_futurism_2022.csv')\n",
    "\n",
    "\n",
    "# Use this code snippet in your app.\n",
    "# If you need more information about configurations\n",
    "# or implementing the sample code, visit the AWS docs:\n",
    "# https://aws.amazon.com/developer/language/python/\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def get_secret():\n",
    "\n",
    "    secret_name = \"RASHI_OPENAI_API_KEY\"\n",
    "    region_name = \"us-east-2\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        # For a list of exceptions thrown, see\n",
    "        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "\n",
    "    # Your code goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=\"Here's a list of 50 identified topics from the provided documents, along with at least 5 keywords corresponding to each topic:\\n\\n1. **Elon Musk's Ventures**\\n   - Keywords: Tesla, SpaceX, Twitter, CEO, acquisition\\n   \\n2. **AI and Chatbots**\\n   - Keywords: LaMDA, Blake Lemoine, sentience, deep learning, conversation\\n\\n3. **NFTs and Digital Art**\\n   - Keywords: Bored Ape, digital collectibles, blockchain, ownership, cryptocurrency\\n\\n4. **Space Exploration**\\n   - Keywords: NASA, Artemis, Mars, Moon, lunar missions\\n   \\n5. **Climate Change Impacts**\\n   - Keywords: environmental crisis, warming, pollution, megadrought, global warming\\n\\n6. **Fusion Energy**\\n   - Keywords: fusion reactors, energy gain, sustainable power, Lawrence Livermore, climate crisis\\n\\n7. **Astronomy Discoveries**\\n   - Keywords: exoplanets, cosmic structure, black holes, dark matter, telescope\\n\\n8. **Smart Technology and Gadgets**\\n   - Keywords: smartwatches, home automation, IoT, wireless technology, fitness tracking\\n\\n9. **Electric Vehicles**\\n   - Keywords: Tesla, EV market, battery technology, sustainability, charging\\n\\n10. **Cybersecurity Issues**\\n    - Keywords: data privacy, hacking, surveillance, cryptocurrency, cyber threats\\n\\n11. **Modern Education Solutions**\\n    - Keywords: online learning, digital tools, remote education, engagement, technology in education\\n\\n12. **Social Media Challenges**\\n    - Keywords: misinformation, content moderation, free speech, platforms, public perception\\n\\n13. **Wildlife and Nature**\\n    - Keywords: conservation, biodiversity, extinction, ecosystem, sustainable practices\\n\\n14. **Public Health and Well-being**\\n    - Keywords: mental health, wellness, healthcare technology, nutrition, community health\\n\\n15. **Economic Policies and Regulations**\\n    - Keywords: cryptocurrency regulation, inflation, fiscal policy, monetary issues, blockchain\\n\\n16. **Consumer Technology Trends**\\n    - Keywords: 3D printing, gaming, e-commerce, digital transformations, product reviews\\n\\n17. **Personal Finance and Investment**\\n    - Keywords: stock market, investing strategies, financial literacy, crypto market, savings\\n\\n18. **Urban Development and Innovation**\\n    - Keywords: smart cities, transportation, infrastructure, sustainability, technology in urban planning\\n\\n19. **Art and Cultural Critique**\\n    - Keywords: art movements, cultural narratives, NFT critiques, modern art, artistic innovation\\n\\n20. **Ethics in Technology**\\n    - Keywords: privacy concerns, artificial intelligence, technology ethics, moral implications, data handling\\n\\n21. **Gaming Industry Dynamics**\\n    - Keywords: video games, gaming consoles, esports, user experience, market trends\\n\\n22. **Food Technology and Sustainability**\\n    - Keywords: lab-grown meat, sustainable agriculture, food supply chain, nutrition, innovations in food\\n\\n23. **Environment and Sustainability**\\n    - Keywords: renewable energy, environmental initiatives, green products, sustainable practices, climate action\\n\\n24. **Digital Communication and Safety**\\n    - Keywords: video calls, remote work, communication technology, digital interaction, privacy\\n\\n25. **Transportation Innovations**\\n    - Keywords: flying taxis, aerospace technology, urban mobility, drone deliveries, electric vehicles\\n\\n26. **Retail and Consumer Behavior**\\n    - Keywords: e-commerce, shopping trends, consumer electronics, advertising, market analysis\\n\\n27. **Parenting in the Digital Age**\\n    - Keywords: child safety, family dynamics, technology use, healthy habits, education tools\\n\\n28. **Space Debris and Safety**\\n    - Keywords: space junk, satellite collisions, debris management, astronaut safety, space exploration\\n\\n29. **Public Response to Climate Issues**\\n    - Keywords: citizen engagement, climate movements, social responsibility, grassroots initiatives, activism\\n\\n30. **Scientific Discoveries and Innovations**\\n    - Keywords: research breakthroughs, laboratory findings, medical research, technology advancement, scientific community\\n\\n31. **Fiction and Reality in Media**\\n    - Keywords: storytelling, narrative forms, virtual realities, media influence, ethics in media\\n\\n32. **Cultural Responses to Technology**\\n    - Keywords: societal change, technology adoption, cultural shifts, collective behavior, values\\n\\n33. **Public Safety and Oversight**\\n    - Keywords: community safety, law enforcement, drug enforcement, public health, surveillance \\n\\n34. **Controversies in Celebrity Culture**\\n    - Keywords: public image, celebrity behavior, social media, public relations, controversies\\n\\n35. **Video Gaming and Society**\\n    - Keywords: gaming culture, social engagement, community building, gaming ethics, youth culture\\n\\n36. **Cryptocurrency and Financial Systems**\\n    - Keywords: Bitcoin, financial technology, blockchain applications, digital currencies, economic impact\\n\\n37. **Gaming and Competition**\\n    - Keywords: competitive gaming, esports, player engagement, gaming strategies, community events\\n\\n38. **Advanced Medical Technologies**\\n    - Keywords: telemedicine, biotechnology, healthcare innovations, medical research, patient care\\n\\n39. **Artificial Intelligence Applications**\\n    - Keywords: machine learning, automation, human-computer interaction, AI tools, data analysis\\n\\n40. **Privacy in the Digital Age**\\n    - Keywords: data protection, individual rights, cyber threats, information security, privacy legislation\\n\\n41. **Climate Justice and Advocacy**\\n    - Keywords: environmental justice, equity, policy advocacy, climate activism, underrepresented communities\\n\\n42. **Transportation Infrastructure Changes**\\n    - Keywords: public transportation, infrastructure development, electric vehicles, urban challenges, mobility solutions\\n\\n43. **Sustainable Food Systems**\\n    - Keywords: food security, agriculture technology, sustainable practices, GMOs, nutrition\\n\\n44. **Geopolitical Tensions and Technology**\\n    - Keywords: international relations, technological competition, geopolitical issues, national security, space partnerships \\n\\n45. **Digital Collector's Items**\\n    - Keywords: NFTs, digital ownership, collectibles, market value, authentication\\n\\n46. **Smart Home Adoption Trends**\\n    - Keywords: home automation, technology integration, digital lifestyle, security systems, user convenience\\n\\n47. **Youth Engagement in Environmental Issues**\\n    - Keywords: education, advocacy, sustainability practices, youth mobilization, green initiatives\\n\\n48. **Perception of Space and Time in Physics**\\n    - Keywords: quantum physics, cosmology, space exploration, scientific theories, relativity\\n\\n49. **Social Issues in Technology Use**\\n    - Keywords: digital citizenship, online behavior, social impact, ethical standards, public awareness\\n\\n50. **Cultural Heritage Preservation**\\n    - Keywords: historical artifacts, archaeological findings, heritage initiatives, cultural identity, conservation\\n\\nThese topics have been organized to reflect the diverse range of subjects covered within the documents, ensuring minimal overlap among the identified topics.\"\n",
    "\n",
    "with open('E:\\\\Rashi\\\\octis\\\\llm_output.txt','w') as file:\n",
    "    file.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('E:\\\\Rashi\\\\octis\\\\LDAtomopy_HOCoherence_news20Results.csv')\n",
    "df.sort_values(by='Median(model_runs)',inplace=True, ascending=False)\n",
    "df.to_csv('E:\\\\Rashi\\\\octis\\\\LDAtomopy_HOCoherence_news20Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intra-Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:\\\\Rashi\\\\datasets\\\\sci.space.txt\\\\sci.space.txt\",'r') as file:\n",
    "    sci_space=file.read()\n",
    "\n",
    "sci_space_documents=sci_space.split('Newsgroup: sci.space')[1:]\n",
    "\n",
    "i=0\n",
    "while i<len(sci_space_documents):\n",
    "    try:\n",
    "        idx=sci_space_documents[i].index('Subject')\n",
    "        sci_space_documents[i]=sci_space_documents[i][idx:].replace('\\n',' ').replace('\\t',' ')\n",
    "    except:\n",
    "        print('breaks it')\n",
    "        break\n",
    "    i+=1\n",
    "\n",
    "sci_space_documents_merged='\\n'.join(sci_space_documents)\n",
    "with open(\"E:\\\\Rashi\\\\datasets\\\\sci.space.txt\\\\sci.space.processed.txt\",'w') as file:\n",
    "    file.write(sci_space_documents_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1974/1974 [01:00<00:00, 32.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab created. vocab size:  13625\n"
     ]
    }
   ],
   "source": [
    "#preprocessData\n",
    "import sys\n",
    "if 'E:\\\\Rashi\\\\octis\\\\OCTIS' not in sys.path: sys.path.append('E:\\\\Rashi\\\\octis\\\\OCTIS') \n",
    "\n",
    "\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "import spacy, string\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import stop_words\n",
    "stop_words=stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "stop_words=list(stop_words)+['share','article','futurism','d', 'll', 'm', 'n', 's', 't', 've']\n",
    "preprocessor = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list=stop_words,\n",
    "                             min_chars=1, min_words_docs=50, encoding='latin-1')\n",
    "\n",
    "dataset = preprocessor.preprocess_dataset(documents_path=\"E:\\\\Rashi\\\\datasets\\\\sci.space.txt\\\\sci.space.processed.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"E:\\\\Rashi\\\\octis\\\\helperFiles\\\\sci.space.octisProcessedDataset.pkl\",'wb') as file:\n",
    "    pickle.dump(dataset,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on NMF in module sklearn.decomposition._nmf object:\n",
      "\n",
      "class NMF(_BaseNMF)\n",
      " |  NMF(n_components='warn', *, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, verbose=0, shuffle=False)\n",
      " |  \n",
      " |  Non-Negative Matrix Factorization (NMF).\n",
      " |  \n",
      " |  Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\n",
      " |  whose product approximates the non-negative matrix X. This factorization can be used\n",
      " |  for example for dimensionality reduction, source separation or topic extraction.\n",
      " |  \n",
      " |  The objective function is:\n",
      " |  \n",
      " |      .. math::\n",
      " |  \n",
      " |          L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n",
      " |  \n",
      " |          &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n",
      " |  \n",
      " |          &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n",
      " |  \n",
      " |          &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n",
      " |  \n",
      " |          &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2\n",
      " |  \n",
      " |  Where:\n",
      " |  \n",
      " |  :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n",
      " |  \n",
      " |  :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n",
      " |  \n",
      " |  The generic norm :math:`||X - WH||_{loss}` may represent\n",
      " |  the Frobenius norm or another supported beta-divergence loss.\n",
      " |  The choice between options is controlled by the `beta_loss` parameter.\n",
      " |  \n",
      " |  The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n",
      " |  `H` to keep their impact balanced with respect to one another and to the data fit\n",
      " |  term as independent as possible of the size `n_samples` of the training set.\n",
      " |  \n",
      " |  The objective function is minimized with an alternating minimization of W\n",
      " |  and H.\n",
      " |  \n",
      " |  Note that the transformed data is named W and the components matrix is named H. In\n",
      " |  the NMF literature, the naming convention is usually the opposite since the data\n",
      " |  matrix X is transposed.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <NMF>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int or {'auto'} or None, default=None\n",
      " |      Number of components, if n_components is not set all features\n",
      " |      are kept.\n",
      " |      If `n_components='auto'`, the number of components is automatically inferred\n",
      " |      from W or H shapes.\n",
      " |  \n",
      " |      .. versionchanged:: 1.4\n",
      " |          Added `'auto'` value.\n",
      " |  \n",
      " |  init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n",
      " |      Method used to initialize the procedure.\n",
      " |      Valid options:\n",
      " |  \n",
      " |      - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n",
      " |        otherwise random.\n",
      " |  \n",
      " |      - `'random'`: non-negative random matrices, scaled with:\n",
      " |        `sqrt(X.mean() / n_components)`\n",
      " |  \n",
      " |      - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n",
      " |        initialization (better for sparseness)\n",
      " |  \n",
      " |      - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n",
      " |        (better when sparsity is not desired)\n",
      " |  \n",
      " |      - `'nndsvdar'` NNDSVD with zeros filled with small random values\n",
      " |        (generally faster, less accurate alternative to NNDSVDa\n",
      " |        for when sparsity is not desired)\n",
      " |  \n",
      " |      - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          When `init=None` and n_components is less than n_samples and n_features\n",
      " |          defaults to `nndsvda` instead of `nndsvd`.\n",
      " |  \n",
      " |  solver : {'cd', 'mu'}, default='cd'\n",
      " |      Numerical solver to use:\n",
      " |  \n",
      " |      - 'cd' is a Coordinate Descent solver.\n",
      " |      - 'mu' is a Multiplicative Update solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Coordinate Descent solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         Multiplicative Update solver.\n",
      " |  \n",
      " |  beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n",
      " |      Beta divergence to be minimized, measuring the distance between X\n",
      " |      and the dot product WH. Note that values different from 'frobenius'\n",
      " |      (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n",
      " |      fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n",
      " |      matrix X cannot contain zeros. Used only in 'mu' solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance of the stopping condition.\n",
      " |  \n",
      " |  max_iter : int, default=200\n",
      " |      Maximum number of iterations before timing out.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Used for initialisation (when ``init`` == 'nndsvdar' or\n",
      " |      'random'), and in Coordinate Descent. Pass an int for reproducible\n",
      " |      results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  alpha_W : float, default=0.0\n",
      " |      Constant that multiplies the regularization terms of `W`. Set it to zero\n",
      " |      (default) to have no regularization on `W`.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  alpha_H : float or \"same\", default=\"same\"\n",
      " |      Constant that multiplies the regularization terms of `H`. Set it to zero to\n",
      " |      have no regularization on `H`. If \"same\" (default), it takes the same value as\n",
      " |      `alpha_W`.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  l1_ratio : float, default=0.0\n",
      " |      The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n",
      " |      For l1_ratio = 0 the penalty is an elementwise L2 penalty\n",
      " |      (aka Frobenius Norm).\n",
      " |      For l1_ratio = 1 it is an elementwise L1 penalty.\n",
      " |      For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Regularization parameter *l1_ratio* used in the Coordinate Descent\n",
      " |         solver.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Whether to be verbose.\n",
      " |  \n",
      " |  shuffle : bool, default=False\n",
      " |      If true, randomize the order of coordinates in the CD solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *shuffle* parameter used in the Coordinate Descent solver.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : ndarray of shape (n_components, n_features)\n",
      " |      Factorization matrix, sometimes called 'dictionary'.\n",
      " |  \n",
      " |  n_components_ : int\n",
      " |      The number of components. It is same as the `n_components` parameter\n",
      " |      if it was given. Otherwise, it will be same as the number of\n",
      " |      features.\n",
      " |  \n",
      " |  reconstruction_err_ : float\n",
      " |      Frobenius norm of the matrix difference, or beta-divergence, between\n",
      " |      the training data ``X`` and the reconstructed data ``WH`` from\n",
      " |      the fitted model.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Actual number of iterations.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DictionaryLearning : Find a dictionary that sparsely encodes data.\n",
      " |  MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n",
      " |  PCA : Principal component analysis.\n",
      " |  SparseCoder : Find a sparse representation of data from a fixed,\n",
      " |      precomputed dictionary.\n",
      " |  SparsePCA : Sparse Principal Components Analysis.\n",
      " |  TruncatedSVD : Dimensionality reduction using truncated SVD.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n",
      " |     factorizations\" <10.1587/transfun.E92.A.708>`\n",
      " |     Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n",
      " |     of electronics, communications and computer sciences 92.3: 708-721, 2009.\n",
      " |  \n",
      " |  .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n",
      " |     beta-divergence\" <10.1162/NECO_a_00168>`\n",
      " |     Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
      " |  >>> from sklearn.decomposition import NMF\n",
      " |  >>> model = NMF(n_components=2, init='random', random_state=0)\n",
      " |  >>> W = model.fit_transform(X)\n",
      " |  >>> H = model.components_\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      NMF\n",
      " |      _BaseNMF\n",
      " |      sklearn.base.ClassNamePrefixFeaturesOutMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components='warn', *, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, verbose=0, shuffle=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, W=None, H=None)\n",
      " |      Learn a NMF model for the data X and returns the transformed data.\n",
      " |      \n",
      " |      This is more efficient than calling fit followed by transform.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present for API consistency by convention.\n",
      " |      \n",
      " |      W : array-like of shape (n_samples, n_components), default=None\n",
      " |          If `init='custom'`, it is used as initial guess for the solution.\n",
      " |          If `None`, uses the initialisation method specified in `init`.\n",
      " |      \n",
      " |      H : array-like of shape (n_components, n_features), default=None\n",
      " |          If `init='custom'`, it is used as initial guess for the solution.\n",
      " |          If `None`, uses the initialisation method specified in `init`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      W : ndarray of shape (n_samples, n_components)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform the data X according to the fitted NMF model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      W : ndarray of shape (n_samples, n_components)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNMF:\n",
      " |  \n",
      " |  fit(self, X, y=None, **params)\n",
      " |      Learn a NMF model for the data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present for API consistency by convention.\n",
      " |      \n",
      " |      **params : kwargs\n",
      " |          Parameters (keyword arguments) and values passed to\n",
      " |          the fit_transform instance.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  inverse_transform(self, Xt=None, W=None)\n",
      " |      Transform data back to its original space.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)\n",
      " |          Transformed data matrix.\n",
      " |      \n",
      " |      W : deprecated\n",
      " |          Use `Xt` instead.\n",
      " |      \n",
      " |          .. deprecated:: 1.3\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          Returns a data matrix of the original shape.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassNamePrefixFeaturesOutMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      The feature names out will prefixed by the lowercased class name. For\n",
      " |      example, if the transformer outputs 3 features, then the feature names\n",
      " |      out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Only used to validate feature names with the names seen in `fit`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassNamePrefixFeaturesOutMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |      \n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |      \n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |      \n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf=NMF()\n",
    "help(nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "if 'E:\\\\Rashi\\\\octis\\\\OCTIS' not in sys.path: sys.path.append('E:\\\\Rashi\\\\octis\\\\OCTIS') \n",
    "\n",
    "# # readPickleFile\n",
    "with open('E:\\\\Rashi\\\\octis\\\\helperFiles\\\\futurism_2022_content_preprocessed.pkl','rb') as file:\n",
    "    fut_dataset=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=fut_dataset.get_corpus()\n",
    "pd.DataFrame(corpus).to_csv('E:\\\\Rashi\\\\octis\\\\OCTIS\\\\preprocessed_datasets\\\\Futurism\\\\corpus.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fut_dataset.get_metadata()\n",
    "\n",
    "with open('E:\\\\Rashi\\\\octis\\\\OCTIS\\\\preprocessed_datasets\\\\Futurism\\\\metadata.json','w') as jfile:\n",
    "    json.dump(fut_dataset.get_metadata(), jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_dataset.get_labels()\n",
    "\n",
    "with open('E:\\\\Rashi\\\\octis\\\\OCTIS\\\\preprocessed_datasets\\\\Futurism\\\\labels.txt','w') as file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(fut_dataset.get_vocabulary())\n",
    "\n",
    "with open('E:\\\\Rashi\\\\octis\\\\OCTIS\\\\preprocessed_datasets\\\\Futurism\\\\vocabulary.txt','w',encoding='utf-8') as file:\n",
    "    [file.write(f'{word} \\n') for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=\"E:\\\\Rashi\\\\datasets\\\\articles_apple.pkl\"\n",
    "\n",
    "import pickle\n",
    "with open(file,'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"E:\\\\Rashi\\\\datasets\\\\articles_apple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octis_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
